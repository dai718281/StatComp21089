---
title: "MY-HW"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MY-HW}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## HW1

3.4

从上到下，从左至右的$\sigma$参数值分别为1，2，3，4

```{r }
set.seed(2)
n<-1000
u<-runif(n)
sigma<-c(1,2,3,4)
i<-1
#par(mfrow=c(2,2))
while (i<=4) {
  x<-sqrt(-2*sigma[i]^2*log(1-u))
y<-seq(0,11,0.001)
hist(x,prob=TRUE,main='')
lines(y,(y/sigma[i]^2)*exp(-y^2/(2*sigma[i]^2)))
i<-i+1
}
```
\

3.11

$z1,z2,z3,z4$的$p$参数值分别为0.75,0.625,0.5,0.375

```{r }
set.seed(1)
n<-1e3
x1<-rnorm(n,0,1)
x2<-rnorm(n,3,1)
p1<-0.75
z1<-p1*x1+(1-p1)*x2
p2<-0.625
z2<-p2*x1+(1-p2)*x2
p3<-0.5
z3<-p3*x1+(1-p3)*x2
p4<-0.375
z4<-p4*x1+(1-p4)*x2
#par(mfrow=c(1,2))
hist(z1);hist(z2)
hist(z3);hist(z4)
```

可以观察到当$p=0.5$时出现比较明显的双峰图形
\

3.20

```{r }
lamda<-4   #let lambda=4
n<-numeric(1e5)
for (i in 1:1e5) {
  N<-rpois(1,10*lamda)
  Y<-rgamma(N,5,4)
  n[i]<-sum(Y)
}
c(mean(n),var(n))  #The estimate the mean and the variance of X(10)
```
这与题目中给出的公式所算出的理论值$E[X(10)]=50,Var[X(10)]=75$相符
\
改变上述程序中$\lambda$的取值，可以获得与理论相符的结果

## HW2

5.4

```{r}
set.seed(1)
x<-c(seq(0.1,0.9,0.1))
pbeta33<-beta33<-numeric(9)
for (i in 1:9) {
  y<-runif(1e6,0,x[i])
  beta33[i]<-mean(30*y^2*(1-y)^2*x[i])  #the estimate of F(x)
  pbeta33[i]<-pbeta(x[i],3,3) #the true value of F(x)
}
round(beta33,5) #the estimate of F(x) for x=0.1,0.2,...,0.9
pbeta33 #the true value of F(x) for x=0.1,0.2,...,0.9
```

5.9

```{r}
set.seed(2)
sigma1<-2 #the sigma value of Rayleigh density is 2
x1<-runif(1e5)
x2<-runif(1e5)
x<-c(x1,x2)
y1<-x/sigma1^2*exp(-x*2/(2*sigma1^2)) #y1 are the sample of iid X1,X2
y2<-x1/sigma1^2*exp(-x1*2/(2*sigma1^2))+(1-x1)/sigma1^2*exp(-(1-x1)*2/(2*sigma1^2))
#y2 are the sample of X1 and are generated by antithetic variable
c(sd(y1),sd(y2)) #the variance of y1 and y2
sd(y2)/sd(y1) 
```

5.13

Let $f_1=xe^{1/2-x^2/2}$ and $f_2=1/3x^3e^{1/2-x^2/2}$ that are supported on $(1,+\infty)$ and are 'close' to $g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},x>1$.

Thus we get $\hat\theta_1=\frac1m\sum_{i=1}^m\frac{g(X_i)}{f_1(X_i)}$ where $X_i$ has pdf/pmf $f_1$ and $\hat\theta_2=\frac1m\sum_{i=1}^m\frac{g(X_i)}{f_2(X_i)}$ where $X_i$ has pdf/pmf $f_2$ as 2 estimates of $\theta$.

The variance of $\hat\theta_1=\frac1mvar\frac{g(X_1)}{f_1(X_1)}=\frac1mvar(\frac{x}{\sqrt{2\pi}}e^{-\frac12})=\frac{varx}{2\pi me}=\frac{\int x^3e^{1/2-x^2/2}dx-(\int x^2e^{1/2-x^2/2}dx)^2}{2\pi me}$

and the variance of $\hat\theta_2=\frac1mvar\frac{g(X_1)}{f_2(X_1)}=\frac1mvar(\frac{e^{-\frac12}}{\sqrt{2\pi}x})=\frac{9var\frac1x}{2\pi me}=\frac{\int 3xe^{1/2-x^2/2}dx-(\int x^2e^{1/2-x^2/2}dx)^2}{2\pi me}.$

Then $\hat\theta_1-\hat\theta_2=\int_1^\infty (x^3-3x)e^{1/2-x^2/2}dx=0$

Thus the two importance functions $f_1$ and $f_2$ have equal variance.

5.14

According to 5.13,$f_1=xe^{1/2-x^2/2}$ can be the importance function to close to $g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},x>1$.

Then $\theta=\int_1^\infty \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx$ can be estimated with $\hat\theta=\frac1m\sum_{i=1}^m\frac{g(X_i)}{f_1(X_i)}=\frac1m\sum_{i=1}^m \frac{X_i}{\sqrt{2\pi e}}$,where $X_i$ has i.i.d pdf $f_1,i=1,2,3,...,m$.

```{r}
u<-runif(1e5)
x<-sqrt(1-2*log(u)) #x>1 has pdf f1
theta<-mean(x)/sqrt(2*pi*exp(1))
theta
```

## HW3

6.5

```{r,eval=FALSE}
if(FALSE){
set.seed(1)
n<-20;m<-1e5
a<-numeric(m)
for (i in 1:m) {
  x<-rchisq(20,2) #generate 20 random numbers of chi-square distribution of df 2
  CIInf<-t.test(x)[4]$conf.int[1] #get t-test`s lower bound of CI
  CISup<-t.test(x)[4]$conf.int[2] #get t-test`s upper bound of CI
  a[i]<-mean(CIInf<=2 & 2<=CISup) #if CIInf<=2<=CISup,a[i]=1,else a[i]=0
}
mean(a) # output the coverage probability of the t-interval
}
```
The t-interval result are more robust to departures from normality than the interval  result for variance in Example 6.4

6.A

where the sampled population is chi-square(1),by running the following R-code,we can get the TIe:
```{r}
set.seed(2)
alpha<-0.05;n<-20
m<-1e5;mu<-1
rejectnumber<-numeric(m)
for (i in 1:m) {
  x<-rchisq(n,mu) #generate n random numbers of chi-square distribution of df mu
  pvalue<-t.test(x,mu=1)[3] #get p-value of the t-test of x where mu=1
  rejectnumber[i]<-mean(pvalue<=alpha) #if pvalue<=alpha,
  #rejectnumber[i]=1,else rejectnumber[i]=0
}
TIe<-mean(rejectnumber) #the Type I error rate
TIe
```
Similarly,where the sampled population is U(0,2) and exp(1),TIe are
```{r, echo=FALSE}
set.seed(3)
alpha<-0.05;n<-20
m<-1e5;mu<-1
rejectnumber<-numeric(m)
for (i in 1:m) {
  x<-runif(n,0,2)
  pvalue<-t.test(x,mu=1)[3]
  rejectnumber[i]<-mean(pvalue<=alpha)
}
TIe<-mean(rejectnumber)
TIe
```

```{r, echo=FALSE}
set.seed(4)
alpha<-0.05;n<-20
m<-1e5;mu<-1
rejectnumber<-numeric(m)
for (i in 1:m) {
  x<-rexp(n)
  pvalue<-t.test(x,mu=1)[3]
  rejectnumber[i]<-mean(pvalue<=alpha)
}
TIe<-mean(rejectnumber)
TIe
```

2

(1)The hypothesis is {$H_0:\mu_1-0.05=\mu_2-0.05=0 \leftarrow\rightarrow H_1=\bar{H_0}$},where $\mu_1$ and $\mu_2$ are powers from two different methods.

(2)For the hypothesis test problem,we should use paired-t test because the powers for two methods under a particular simulation setting  maybe not independent. But $\mu_1$ and $\mu_2$ have the same variance and they are both asymptotic normal distribution (CLT) because they are under the same simulation.

(3)For hypothesis testing,we need sample data from each methods to calculate their powers.For example,if we use method I,we need m groups experimental data and each of them include n data,so that we obtain m powers for hypothesis testing.And method II so do.

## HW4

6.8

```{r,eval=FALSE}
library(MASS)
d<-3;m<-1e3
n <- c(10,20,30,50,100,500)
cv<-qchisq(.95,d*(d+1)*(d+2)/6) #critical value for the skewness test
sktests<-numeric(m);p.reject<-numeric(length(n))
#compute b_1d
b_1d<- function(x,j) {
  xbar<-colMeans(x)
  a<-numeric(n[j])
  for (i in 1:n[j]) {
    a[i]<-mean(((x[i,]-xbar)%*%ginv(cov(x,x))%*%t((x-xbar)))^3)
  }
  return(mean(a)) #b_1d=mean(a)
}
#compute p.reject for every n[i]
for (j in 1:length(n)) {
  for (i in 1:m) {
    #generate matrix x where x[i,] are norm(0,1) r.v.
    x <- matrix(rnorm(n[j]*d),nrow=n[j],ncol=d)
    sktests[i]<-as.integer(n[j]*b_1d(x,j)/6>=cv)
  }
  p.reject[j] <- mean(sktests)
}
p.reject
```

Because the skewness of chisquared with d(d + 1)(d + 2)/6 degrees of freedom is not 0, p-value should be small to reject $H_0$. And the result of p.reject above is consistent.

6.10

To improve operational efficiency,we use n[3]=30 as n:

```{r,eval=FALSE}
epsilon <- c(seq(0,0.15,0.01), seq(0.15,1,0.05))
N<-length(epsilon)
pwr<-numeric(N)
for (j in 1:N) { #for each epsilon
  e <- epsilon[j]
  sktests <- numeric(m)
  for (i in 1:m) { #for each replicate
    sigma <- sample(c(1, 10), replace = TRUE,
                    size = n, prob = c(1-e, e))
    #x <- rnorm(n,0,sigma)
    x <- matrix(rnorm(n[3]*d,0,sigma),nrow=n[3],ncol=d)
    sktests[i] <- as.integer(n[3]*b_1d(x,3)/6>= cv)
  }
  pwr[j] <- mean(sktests)
}
#plot power vs epsilon
plot(epsilon, pwr, type = "b",
     xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
```

When $\epsilon\rightarrow0$ or $1$,model is normal, so that p should be similar with 6.8. Else model is not normal, then p will be larger.

## HW5

7.7

```{r}
library(bootstrap);set.seed(1)
eigenvalues<-eigen(cov(scor))$values  #eigenvalues of cov(scor)
theta<-eigenvalues[1]/sum(eigenvalues) #the true theta
m<-1e3;thetahat<-numeric(m)
#bootstrap to compute theta_star
for (i in 1:m) {
  x<-sample(1:88,88,replace = TRUE)
  scorhat<-scor[x,]
  eigenvalueshat<-eigen(cov(scorhat))$values
  thetahat[i]<-eigenvalueshat[1]/sum(eigenvalueshat)
}
thetastar<-mean(thetahat)
round(c(bias=theta-thetastar,se=sd(thetahat)),5)
```

7.8

```{r}
library(bootstrap);set.seed(2)
eigenvalues<-eigen(cov(scor))$values
theta<-eigenvalues[1]/sum(eigenvalues)
scorhat<-matrix(0,nrow = 87,ncol = 5)
thetahat<-numeric(88)
for (i in 1:88) {
  #Jackknife to compute theta_star
  scorhat<-scor[-i,]
  eigenvalueshat<-eigen(cov(scorhat))$values
  thetahat[i]<-eigenvalueshat[1]/sum(eigenvalueshat)
}
thetastar<-mean(thetahat);se<-sqrt(87*mean((thetahat-mean(thetahat))^2))
round(c(bias=87*(theta-thetastar),se=se),5)
```

7.9

```{r}
library(bootstrap);library(boot)
m<-1e4;set.seed(3)
eigenvalues<-eigen(cov(scor))$values
theta<-eigenvalues[1]/sum(eigenvalues)
boot.theta<- function(data,i) {
  boot.eigenvalues<-eigen(cov(data[i,]))$values
  boot.theta<-boot.eigenvalues[1]/sum(boot.eigenvalues)
  return(boot.theta)
}
de <- boot(data=scor,statistic=boot.theta, R =1e4)
ci <- boot.ci(de,type=c("perc","bca"))
cat('perc =',ci$percent[4:5],'BCa =',ci$bca[4:5])
```

There are confidence interval inf and sup of percentile and BCa methods.

7.B

For normal populations,we should compare the sample datas to 0,where is the skewness of nrom(0,1):

```{r,eval=FALSE}
n<-1e2;m<-1e3;library(boot);set.seed(4)
boot.skewness <- function(x,i) mean(((x[i]-mean(x[i]))/sd(x[i]))^3)
ci.norm<-ci.basic<-ci.perc<-matrix(0,m,2)
for(i in 1:m){
  U<-rnorm(n)
  de <- boot(data=U,statistic=boot.skewness, R =1e3)
  ci <- boot.ci(de,type=c("norm","basic","perc"))
  ci.norm[i,]<-ci$norm[2:3];ci.basic[i,]<-ci$basic[4:5]
  ci.perc[i,]<-ci$percent[4:5]
}
cat('norm =',mean(ci.norm[,1]<=0 & ci.norm[,2]>=0),
    'basic =',mean(ci.basic[,1]<=0 & ci.basic[,2]>=0),
    'perc =',mean(ci.perc[,1]<=0 & ci.perc[,2]>=0))
```

The skewness of chisq with df 5 is\
$$E(\frac{X-\mu}{\sigma})^3=\frac{EX^3-3\mu\sigma^2-\mu^3}{\sigma^3}=\frac{40}{10^{3/2}}>0.$$
R-code is similar to the normal population situation:

```{r,eval=FALSE}
mu<-40/10^1.5;n<-1e2;m<-1e3;library(boot);set.seed(5)
boot.skewness <- function(x,i) mean(((x[i]-mean(x[i]))/sd(x[i]))^3)
ci.norm<-ci.basic<-ci.perc<-matrix(0,m,2)
for(i in 1:m){
  U<-rchisq(n,5)
  de <- boot(data=U,statistic=boot.skewness, R =1e3)
  ci <- boot.ci(de,type=c("norm","basic","perc"))
  ci.norm[i,]<-ci$norm[2:3];ci.basic[i,]<-ci$basic[4:5]
  ci.perc[i,]<-ci$percent[4:5]
}
cat('norm =',mean(ci.norm[,1]<=mu & ci.norm[,2]>=mu),
    'basic =',mean(ci.basic[,1]<=mu & ci.basic[,2]>=mu),
    'perc =',mean(ci.perc[,1]<=mu & ci.perc[,2]>=mu))
```

We can find the coverage rates is less than the normal populations.

## HW6

##### Exercise 8.2

```{r}
#data set from chickwts of R-studio
attach(chickwts)
x <- as.vector(weight[feed == "linseed"])
y <- as.vector(weight[feed == "casein"])
detach(chickwts)
z<-c(x,y)
```

The figure of linseed v.s. casein is:

```{r,echo=FALSE}
plot(x,ylab='Weight',ylim=range(z),pch=1)
points(y,col=2,pch=2)
legend(8,410,c('linseed','casein'),col=1:2,pch=1:2)
```

```{r}
R <- 1e3;K <- 1:length(z);n<-length(x);set.seed(1)
reps <- numeric(R);t0 <- cor(x,y,method = 'spearman')
for (i in 1:R) {
  k <- sample(K, size = n, replace = FALSE)
  x1 <- z[k]; y1 <- z[-k] #complement of x1
  reps[i] <- cor(x1,y1,method = 'spearman')
}
p <- mean(abs(c(t0, reps)) >= abs(t0))
round(c(p,cor.test(x,y,method = 'spearman')$p.value),3)
```

##### exercise 2

```{r,eval=FALSE}
library(RANN);library(boot);library(Ball);library(energy)
#Tn function to compute the proportion of the first through Kth NN coincidences
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1)
  block1 <- NN$nn.idx[1:n1,-1] 
  block2 <- NN$nn.idx[(n1+1):n,-1] 
  i1 <- sum(block1 <= n1); i2 <- sum(block2 > n1) 
  (i1 + i2) / (k * n)
}
#eqdist.nn function to compute the p.value of NN-method
eqdist.nn <- function(z,sizes,k){
  boot.obj <- boot(data=z,statistic=Tn,R=R,
  sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}
M <- 1e3; R<-1e3; k<- 2; alpha <- 0.05; set.seed(2)
n1 <- n2 <- 20; p<-3;n <- n1+n2; N = c(n1,n2)
p.values <- matrix(0,M,3)
for (i in 1:4) {
   for(m in 1:M){
      #Unequal variances and equal expectations
      #x obey N(0,1),y obey N(0,1.5)
      if(i==1) {
         x<-matrix(rnorm(n1*p),ncol=p)
         y <-matrix(rnorm(n2*p,0,1.5),ncol=p)
      }
      #Unequal variances and unequal expectations
      #x obey N(1,2),y obey N(2,3)
      else if(i==2) {
         x<-matrix(rnorm(n1*p,1,2),ncol=p)
         y <-matrix(rnorm(n2*p,2,3),ncol=p)
      }
      #Non-normal distributions:
      #x obey t(1),y obey  bimodel distribution (mixture of N(0,1) and N(2,3) with probability 0.3 and 0.7 respectively)
      else if(i==3) {
         r<-sample(c(0,1),n2*p,replace = TRUE,prob = c(0.3,0.7))
         x<-matrix(rt(n1*p,1),ncol=p)
         y <-matrix(r*rnorm(n2*p,0,1)+(1-r)*rnorm(n2*p,2,3),ncol=p)
      }
      #Unbalanced samples:
      #x,y iid obey N(0,1) where x has 10 sanples and y has 30
      else {
         n1 <- 10;n2 <- 30;n <- n1+n2; N = c(n1,n2)
         x <-matrix(rnorm(n1*p),ncol=p)
         y <-matrix(rnorm(n2*p),ncol=p)
      }
      z <- rbind(x,y)
      p.values[m,1] <- eqdist.nn(z,N,k)$p.value
      p.values[m,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
      p.values[m,3] <- bd.test(x=x,y=y,num.permutations=R,seed=m*2)$p.value
   }
   cat('The answer of',i,'is:','\n')
   cat('nn=',mean(p.values[,1]<alpha),
       'energy=',mean(p.values[,2]<alpha),
       'ball-methods=',mean(p.values[,3]<alpha),'\n')
}

```
The result shows energy-method is better than nn-method in most cases,and ball-method sometimes can be better than energy-method.

## HW7

9.3
```{r}
set.seed(10)
m <- 1e4;R<-4;sigma<-c(1,3,5,7);k <- numeric(R)
Y <- matrix(0,nrow = R,ncol = m);Y[,1] <- c(5,10,15,20)
#generate MC
for (r in 1:R) {
  u <- runif(m)
  for (i in 2:m) {
  y <- rnorm(1,Y[r,i-1],sigma[r])
  if (u[i] <= dt(y,df=1)/dt(Y[r,i-1],df=1)){
    Y[r,i] <- y   #y is accept
    } 
  else {
    Y[r,i] <- Y[r,i-1]  #y is rejected
    k[r] <- k[r]+1      #compute the number of rejection
    }
  }
}
```

Here are four different chains with indicators ranging from 8000 to 8500 where unnecessary code is hidden:

```{r,echo=FALSE}
#plot Y[8000] to Y[8500]
index <- 8000:8500
y1 <- Y[,index]
for (r in 1:R){
  if(r==1){
    plot(index,y1[r,], type="l",main="",xlab='Index',
         col=1,ylab="",ylim = c(-10,10))
    }else{
      lines(index,y1[r,], col=r)
    }
}
legend('topleft',c('Sigma=1','Sigma=3','Sigma=5','Sigma=7'),col=1:R,lwd=2)
```

As can be seen from the above figure, the smaller the variance, the faster the convergence of the chain,and conversely,the slower.


```{r,eval=FALSE}
b <- 1000     #discard the first 1000 sample
Ycut <- Y[,(b+1):m]
a <- c(.05, seq(.1, .9, .1), .95)
QSample<-matrix(0,nrow = R,ncol = length(a))
QSCD <- qt(a,df=1)  #true quantiles of standard Cauchy distribution
for (r in 1:R) {
  QSample[r,] <- quantile(Ycut[r,], a) #sample quantiles of discard the first 1000 of the chain
}
#make table of quantiles
tableQ <- data.frame(round(cbind(QSCD,QSample[1,],
                      QSample[2,],QSample[3,],QSample[4,]), 5))
names(tableQ) <- c('True','Sigma=1','Sigma=3','Sigma=5','Sigma=7')
knitr::kable(tableQ,'pipe')
```
We can conclude that the greater the variance, the better the better the quantile fitting effect.

9.8

The bivariate density is $f(x,y)\propto{n \choose x}y^{x+a-1}(1-y)^{n-x+b-1},x=0,1,...,n,0\leq y\leq 1$.
It can be shown that for fixed $a,b,n$, the conditional distributions are $x\sim b(n,y)$ and $y\sim \beta(x+a,n−x+b)$.

```{r}
set.seed(2);N <- 1e4;burn <- 1e3
Z <- matrix(0, N, 2)
a<-4;b<-4;n<-10
###### generate the chain #####
Z[1, ] <- c(sample(1:n,1), runif(1)) #initialize
for (i in 2:N) {
y <- Z[i-1, 2]
Z[i, 1] <- rbinom(1,n,y)
x <- Z[i, 1]
Z[i, 2] <- rbeta(1,x+a,n-x+b)
}
plot(c((8*burn+1):N),Z[(8*burn + 1):N,1],type='l',
     col=1,lwd=2,xlab='Index',ylab='Random numbers')
lines(c((8*burn+1):N),Z[(8*burn + 1):N,2],col=2,lwd=2)
legend('topright',c(expression(x),expression(y)),col=1:2,lwd=2)
```

It can be seen from the above figure that the difference between x and y is obvious, and x fluctuates greatly, while y is relatively stable.

Question 2

```{r}
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)

  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}

```

The following figure shows the chain of four different parameters with the number of iterations from 4000 to 10000 where unnecessary code is hidden.when Sigma is 3 or 5,the convergence effect is good.

```{r,echo=FALSE}
#compute diagnostic statistics
psi <- t(apply(Y, 1, cumsum))
for (i in 1:nrow(psi)){
  psi[i,] <- psi[i,] / (1:ncol(psi))
}

#plot psi for the four chains
for (i in 1:R){
  if(i==1){
    plot((4*burn+1):m,psi[i,(4*burn+1):m],type="l",
            ylim = c(-1,1),xlab='Index', ylab=bquote(phi),col=i)
    }else{
      
      lines((4*burn+1):m,psi[i,(4*burn+1):m], col=i)
    }
}
legend('topleft',c('Sigma=1','Sigma=3','Sigma=5','Sigma=7'),col=1:R,lwd=2)
par(mfrow=c(1,1)) #restore default
```

```{r}
#plot the sequence of R-hat statistics
rhat <- rep(0, m)
for (j in (burn+1):m)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(burn+1):m], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```

MC of 9.3 starts to converge when the number of iterations is about 2000 (the figure discards the first 1000 iterations). The subsequent curve rises as the number of iterations increases, which may be the reason for the random number set.seed().

```{r}
psi <- apply(Z, 1, cumsum)
for (i in 1:nrow(psi)){
  psi[i,] <- psi[i,] / (1:ncol(psi))
}
rhat <- rep(0, m)
for (j in (burn+1):m)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(burn+1):m], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```

R-hat of 9.8 MC is always below 1.2 when the number of iterations is between 1000 and 10000. However its subsequent convergence is still difficult to judge,because the curve is increasing. It requires more in-depth research.

## HW8

11.3

a,b

Here is a function to calculate the sum of the series, where each item of the series is stored in vector f.It can calculate the sum of infinity,since the remainder less than 1e-40 is discarded.

```{r}
sumf <- function(k,d,a) {
  f<-numeric(k+1)
  for (i in 0:k) {
    x1<-(-1)^i/(factorial(i)*2^i)
    x2<-(sum(a^(2*i+2)))^(1/(2*i+2))/(2*i+1)/(2*i+2)
    x3<-gamma((d+1)/2)*gamma(i+3/2)/gamma(i+d/2+1)
    f[i+1]<-x1*x2*x3
    if(f[i+1]<=1e-40) break
  }
  return(sum(f))
}
```

c

Evaluate the sum when $a = (1, 2)^T$.

```{r}
k<-1e4;d<-4;a<-c(1,2)
sumf(k,d,a)
```

11.5

In order to avoid calculating the integral with unknown parameters in the integral limit, we take the derivation of the left and right sides of the equation:
$d(\int_0^{c_{k-1}}(1+\frac{u^2}{k-1})^{-k/2}du)/da=(\frac{k}{k-a^2})^{-k/2}\frac{dc_{k-1}}{da}=\sqrt{\frac{k-1}{k-a}}(1-\frac{a^2}{k})^{k/2-1}$,which is defined as der($\alpha$,k) in the below R-code

```{r}
k<-4
cof<-function(k) {2*exp(lgamma(k/2)-lgamma((k-1)/2))/sqrt(pi*(k-1))}
der<-function(alpha,k) {sqrt((k-1)/(k-alpha^2))*(1-alpha^2/k)^(k/2-1)}
eq<-function(alpha){
  eqleft<-cof(k)*der(alpha = alpha,k=k)
  eqright<-cof(k+1)*der(alpha = alpha,k=k+1)
  eqleft-eqright
}
solution <- uniroot(eq,c(0.2,1.3))
round(unlist(solution),5)[1:3]
```

It is interesting that the equation has more than one solution. When x takes some points in [-2, 2], we can find it has two zero pionts.

```{r}
a<-seq(-2,2,0.2)
for (i in 1:length(a)) {
  print(eq(a[i]))
}
```

question2

Observed likelihood:
$L_o(\lambda|x^1,x^2)=\prod_{i=1}^7 \lambda e^{-\lambda x_i}\prod_{i=1}^3 P(X>1)=\lambda^7e^{-\lambda\sum_{i=1}^7x_i}e^{-3\lambda}$,where $x^1$ is the data of $T_i$ which less than 1 and $x^2$ is the data of $T_i$ which more than 1.

Then $\hat{\lambda}=argmax(log(L_o(\lambda|x^1,x^2)))=\frac{7}{\sum_{i=1}^{7}x_i+3}.$

$L_c(\lambda|x^1,y)=\prod_{i=1}^7 \lambda e^{-\lambda x_i}\prod_{i=1}^3 \lambda e^{-\lambda y_i}=\lambda^{10}e^{-\lambda(\sum_{i=1}^7x_i+\sum_{i=1}^3y_i)}$,where $y$ is the data of $T_i$ which more than 1.

Then $\hat{\lambda}=argmax(log(L_c(\lambda|x^1,y)))=\frac{10}{\sum_{i=1}^{7}x_i+\sum_{i=1}^{3}y_i}.$

E-step:

$E_{\hat{\lambda_0}}[logL_o(\lambda|x^1,y)|x^1,x^2]=10log\lambda-\lambda\sum_{i=1}^7x_i-3\lambda E_{\hat{\lambda_0}}[y|x^2]$. $\hat{\lambda_0}$ is a given initial value and $E_{\hat{\lambda_0}}[y|x^2]=\frac{\int_1^{+\infty}\hat{\lambda_0}e^{-\hat{\lambda_0}y}ydy}{\int_1^{+\infty}\hat{\lambda_0}e^{-\hat{\lambda_0}y}dy}=\frac{\hat{\lambda_0}+1}{\hat{\lambda_0}}$.

M-step:

Let $E_{\hat{\lambda_0}}[logL_o(\lambda|x^1,y)|x^1,x^2]=0$,it shows that $\hat{\lambda_1}=\frac{10}{\sum_{i=1}^{7}x_i+3\frac{\hat{\lambda_0}+1}{\hat{\lambda_0}}}$

The following R-code show $\hat{\lambda_n}\rightarrow\hat{\lambda}$ when $n\rightarrow +\infty$:

```{r}
N<-1e3;tol <- .Machine$double.eps^0.5
y<-c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)
yhat<-c(0.54, 0.48, 0.33, 0.43, 0.91, 0.21, 0.85)
lambda<-7/sum(y);lambdahat<-1.5;lambdahat.old<-lambdahat+1
for (j in 1:N) {
  lambdahat<-10/(sum(yhat)+3*(lambdahat+1)/lambdahat)
  if (sum(abs(lambdahat - lambdahat.old)/lambdahat.old) < tol) break
  lambdahat.old <- lambdahat
}
print(list(lambda=lambda, lambdahat = lambdahat, iter = j, bias = tol))
```

## HW9

Page 204 exercise 1:

```{r}
set.seed(1)
trims <- c(0, 0.1, 0.2, 0.5)
x <- rcauchy(100)
cbind(lapply(trims, function(trim) mean(x, trim = trim)),  #code 1
lapply(trims, mean, x = x))  #code 2
```
    In code 1, it defines a function with parameter trim to calculate the mean of x, where the fraction (0 to 0.5) of observations to be trimmed from each end of x. Then use lapply() to pass each column of trims to f where trim=trims<-c(0, 0.1, 0.2, 0.5) and returns a new list.
    In code 2,the function is mean() where its parameter x=x<-rcauchy(100) is a 100 cauchy distribution random numbers list. Then use lapply() to pass each column of trims to mean and returns a new list.

Page 204 exercise 5:

The following code shows the rsq of the four fitting results:

```{r}
formulas <- list(mpg ~ disp,mpg ~ I(1 / disp),
                 mpg ~ disp + wt,mpg ~ I(1 / disp) + wt) #Four fitting formulas
lms<-lapply(formulas,lm,data=mtcars) #Four parameter results of fitting
rsq <- function(mod) summary(mod)$r.squared #Four r.squared
rsqs<-rbind(lapply(lms,rsq))
rsqs
```

Generate ten different bootstrap samples for mpg~disp and calculate their r-squared:

```{r}
set.seed(2)
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})
lms.boot<-lapply(bootstraps,function(data) lm(mpg ~ disp,data = data))
rsqs<-rbind(lapply(lms.boot,rsq))
rsqs
```

Page 214 exercise 1:

(a)Data frame X has five columns of data, each of them contains 500 random numbers which follows N(0,1)

```{r}
set.seed(3)
X<-data.frame(replicate(5,rnorm(5e2)))
round(vapply(X,sd,
          FUN.VALUE=c(sd=0)),5)
```

(b)Data frame Y has five columns of data, each of them contains 500 random numbers which are random sample from 1 to 20 and missing value NA:

```{r}
set.seed(4)
summary <- function(x) {
  funs <- c(sd)
  sapply(funs, function(f) f(x, na.rm = TRUE))
}
Y<-data.frame(replicate(5,sample(c(1:20,NA),5e2,rep=T)))
round(vapply(Y,summary,
          FUN.VALUE=c(sd=0)),3)
```

Page 214 exercise 7:

In windows situation,we implement parSapply() as a alternative version of mcsapply():

```{r}
library(parallel)
cl <- makeCluster(getOption("cl.cores", 2))
xx <- 1
clusterExport(cl, "xx")
parSapply(cl, 1:20, get("+"), 3)
```

Implement mcvapply() will be hard because the initial value is difficult to assume in high-dimensional situations.

## HW10

The following figure shows the image of the chain generated with C code in the number of iterations from 8000 to 10000,where $x\sim b(n,y)$ and $y\sim \beta(x+a,n−x+b)$.

```{r,eval=FALSE}
set.seed(1);library(Rcpp)
N <- 1e4;a<-4;b<-4;n<-10 #Initial value
source('chainR.R')
sourceCpp('chainC.cpp')
chainsC<-chainC(N,a,b,n);chainsR<-chainR(N,a,b,n)
#plot part of image
burn<-N/10
plot(c((8*burn+1):N),chainsC[(8*burn + 1):N,1],type='l',
     col=1,lwd=2,xlab='Index',ylab='Random numbers')
lines(c((8*burn+1):N),chainsC[(8*burn + 1):N,2],col=2,lwd=2)
legend('topright',c(expression(x),expression(y)),col=1:2,lwd=2)
```

Plot QQ-plot,the following figure shows that chains generated by C and R have high similarity.The fitting effect of Figure (a) is worse, which may be because x follows a discrete distribution

```{r,eval=FALSE}
par(mfrow=c(1,2))
#different x generated by C and R respectively
qqplot(chainsC[,1],chainsR[,1],ylab = 'R_x',xlab = 'C_x',main='(a)')
#different y generated by C and R respectively
qqplot(chainsC[,2],chainsR[,2],ylab = 'R_y',xlab = 'C_y',main='(b)')
```

The following results shows different time spent by the two programs,where R-code has higher efficiency.

```{r,eval=FALSE}
library(microbenchmark) #My R version is 4.1.3
ts <- microbenchmark(chainR=chainR(N,a,b,n), chainC=chainC(N,a,b,n))
summary(ts)[,c(1,3,5,6)]
```

Conclusion:There is no essential difference between the chains generated by the two programs, but the C chain runs more efficiently.