## ----setup, include=FALSE-----------------------------------------------------
knitr::opts_chunk$set(echo = TRUE)

## -----------------------------------------------------------------------------
set.seed(2)
n<-1000
u<-runif(n)
sigma<-c(1,2,3,4)
i<-1
#par(mfrow=c(2,2))
while (i<=4) {
  x<-sqrt(-2*sigma[i]^2*log(1-u))
y<-seq(0,11,0.001)
hist(x,prob=TRUE,main='')
lines(y,(y/sigma[i]^2)*exp(-y^2/(2*sigma[i]^2)))
i<-i+1
}

## -----------------------------------------------------------------------------
set.seed(1)
n<-1e3
x1<-rnorm(n,0,1)
x2<-rnorm(n,3,1)
p1<-0.75
z1<-p1*x1+(1-p1)*x2
p2<-0.625
z2<-p2*x1+(1-p2)*x2
p3<-0.5
z3<-p3*x1+(1-p3)*x2
p4<-0.375
z4<-p4*x1+(1-p4)*x2
#par(mfrow=c(1,2))
hist(z1);hist(z2)
hist(z3);hist(z4)

## -----------------------------------------------------------------------------
lamda<-4   #let lambda=4
n<-numeric(1e5)
for (i in 1:1e5) {
  N<-rpois(1,10*lamda)
  Y<-rgamma(N,5,4)
  n[i]<-sum(Y)
}
c(mean(n),var(n))  #The estimate the mean and the variance of X(10)

## -----------------------------------------------------------------------------
set.seed(1)
x<-c(seq(0.1,0.9,0.1))
pbeta33<-beta33<-numeric(9)
for (i in 1:9) {
  y<-runif(1e6,0,x[i])
  beta33[i]<-mean(30*y^2*(1-y)^2*x[i])  #the estimate of F(x)
  pbeta33[i]<-pbeta(x[i],3,3) #the true value of F(x)
}
round(beta33,5) #the estimate of F(x) for x=0.1,0.2,...,0.9
pbeta33 #the true value of F(x) for x=0.1,0.2,...,0.9

## -----------------------------------------------------------------------------
set.seed(2)
sigma1<-2 #the sigma value of Rayleigh density is 2
x1<-runif(1e5)
x2<-runif(1e5)
x<-c(x1,x2)
y1<-x/sigma1^2*exp(-x*2/(2*sigma1^2)) #y1 are the sample of iid X1,X2
y2<-x1/sigma1^2*exp(-x1*2/(2*sigma1^2))+(1-x1)/sigma1^2*exp(-(1-x1)*2/(2*sigma1^2))
#y2 are the sample of X1 and are generated by antithetic variable
c(sd(y1),sd(y2)) #the variance of y1 and y2
sd(y2)/sd(y1) 

## -----------------------------------------------------------------------------
u<-runif(1e5)
x<-sqrt(1-2*log(u)) #x>1 has pdf f1
theta<-mean(x)/sqrt(2*pi*exp(1))
theta

## ----eval=FALSE---------------------------------------------------------------
#  if(FALSE){
#  set.seed(1)
#  n<-20;m<-1e5
#  a<-numeric(m)
#  for (i in 1:m) {
#    x<-rchisq(20,2) #generate 20 random numbers of chi-square distribution of df 2
#    CIInf<-t.test(x)[4]$conf.int[1] #get t-test`s lower bound of CI
#    CISup<-t.test(x)[4]$conf.int[2] #get t-test`s upper bound of CI
#    a[i]<-mean(CIInf<=2 & 2<=CISup) #if CIInf<=2<=CISup,a[i]=1,else a[i]=0
#  }
#  mean(a) # output the coverage probability of the t-interval
#  }

## -----------------------------------------------------------------------------
set.seed(2)
alpha<-0.05;n<-20
m<-1e5;mu<-1
rejectnumber<-numeric(m)
for (i in 1:m) {
  x<-rchisq(n,mu) #generate n random numbers of chi-square distribution of df mu
  pvalue<-t.test(x,mu=1)[3] #get p-value of the t-test of x where mu=1
  rejectnumber[i]<-mean(pvalue<=alpha) #if pvalue<=alpha,
  #rejectnumber[i]=1,else rejectnumber[i]=0
}
TIe<-mean(rejectnumber) #the Type I error rate
TIe

## ---- echo=FALSE--------------------------------------------------------------
set.seed(3)
alpha<-0.05;n<-20
m<-1e5;mu<-1
rejectnumber<-numeric(m)
for (i in 1:m) {
  x<-runif(n,0,2)
  pvalue<-t.test(x,mu=1)[3]
  rejectnumber[i]<-mean(pvalue<=alpha)
}
TIe<-mean(rejectnumber)
TIe

## ---- echo=FALSE--------------------------------------------------------------
set.seed(4)
alpha<-0.05;n<-20
m<-1e5;mu<-1
rejectnumber<-numeric(m)
for (i in 1:m) {
  x<-rexp(n)
  pvalue<-t.test(x,mu=1)[3]
  rejectnumber[i]<-mean(pvalue<=alpha)
}
TIe<-mean(rejectnumber)
TIe

## ----eval=FALSE---------------------------------------------------------------
#  library(MASS)
#  d<-3;m<-1e3
#  n <- c(10,20,30,50,100,500)
#  cv<-qchisq(.95,d*(d+1)*(d+2)/6) #critical value for the skewness test
#  sktests<-numeric(m);p.reject<-numeric(length(n))
#  #compute b_1d
#  b_1d<- function(x,j) {
#    xbar<-colMeans(x)
#    a<-numeric(n[j])
#    for (i in 1:n[j]) {
#      a[i]<-mean(((x[i,]-xbar)%*%ginv(cov(x,x))%*%t((x-xbar)))^3)
#    }
#    return(mean(a)) #b_1d=mean(a)
#  }
#  #compute p.reject for every n[i]
#  for (j in 1:length(n)) {
#    for (i in 1:m) {
#      #generate matrix x where x[i,] are norm(0,1) r.v.
#      x <- matrix(rnorm(n[j]*d),nrow=n[j],ncol=d)
#      sktests[i]<-as.integer(n[j]*b_1d(x,j)/6>=cv)
#    }
#    p.reject[j] <- mean(sktests)
#  }
#  p.reject

## ----eval=FALSE---------------------------------------------------------------
#  epsilon <- c(seq(0,0.15,0.01), seq(0.15,1,0.05))
#  N<-length(epsilon)
#  pwr<-numeric(N)
#  for (j in 1:N) { #for each epsilon
#    e <- epsilon[j]
#    sktests <- numeric(m)
#    for (i in 1:m) { #for each replicate
#      sigma <- sample(c(1, 10), replace = TRUE,
#                      size = n, prob = c(1-e, e))
#      #x <- rnorm(n,0,sigma)
#      x <- matrix(rnorm(n[3]*d,0,sigma),nrow=n[3],ncol=d)
#      sktests[i] <- as.integer(n[3]*b_1d(x,3)/6>= cv)
#    }
#    pwr[j] <- mean(sktests)
#  }
#  #plot power vs epsilon
#  plot(epsilon, pwr, type = "b",
#       xlab = bquote(epsilon), ylim = c(0,1))
#  abline(h = .1, lty = 3)
#  se <- sqrt(pwr * (1-pwr) / m) #add standard errors
#  lines(epsilon, pwr+se, lty = 3)
#  lines(epsilon, pwr-se, lty = 3)

## -----------------------------------------------------------------------------
library(bootstrap);set.seed(1)
eigenvalues<-eigen(cov(scor))$values  #eigenvalues of cov(scor)
theta<-eigenvalues[1]/sum(eigenvalues) #the true theta
m<-1e3;thetahat<-numeric(m)
#bootstrap to compute theta_star
for (i in 1:m) {
  x<-sample(1:88,88,replace = TRUE)
  scorhat<-scor[x,]
  eigenvalueshat<-eigen(cov(scorhat))$values
  thetahat[i]<-eigenvalueshat[1]/sum(eigenvalueshat)
}
thetastar<-mean(thetahat)
round(c(bias=theta-thetastar,se=sd(thetahat)),5)

## -----------------------------------------------------------------------------
library(bootstrap);set.seed(2)
eigenvalues<-eigen(cov(scor))$values
theta<-eigenvalues[1]/sum(eigenvalues)
scorhat<-matrix(0,nrow = 87,ncol = 5)
thetahat<-numeric(88)
for (i in 1:88) {
  #Jackknife to compute theta_star
  scorhat<-scor[-i,]
  eigenvalueshat<-eigen(cov(scorhat))$values
  thetahat[i]<-eigenvalueshat[1]/sum(eigenvalueshat)
}
thetastar<-mean(thetahat);se<-sqrt(87*mean((thetahat-mean(thetahat))^2))
round(c(bias=87*(theta-thetastar),se=se),5)

## -----------------------------------------------------------------------------
library(bootstrap);library(boot)
m<-1e4;set.seed(3)
eigenvalues<-eigen(cov(scor))$values
theta<-eigenvalues[1]/sum(eigenvalues)
boot.theta<- function(data,i) {
  boot.eigenvalues<-eigen(cov(data[i,]))$values
  boot.theta<-boot.eigenvalues[1]/sum(boot.eigenvalues)
  return(boot.theta)
}
de <- boot(data=scor,statistic=boot.theta, R =1e4)
ci <- boot.ci(de,type=c("perc","bca"))
cat('perc =',ci$percent[4:5],'BCa =',ci$bca[4:5])

## ----eval=FALSE---------------------------------------------------------------
#  n<-1e2;m<-1e3;library(boot);set.seed(4)
#  boot.skewness <- function(x,i) mean(((x[i]-mean(x[i]))/sd(x[i]))^3)
#  ci.norm<-ci.basic<-ci.perc<-matrix(0,m,2)
#  for(i in 1:m){
#    U<-rnorm(n)
#    de <- boot(data=U,statistic=boot.skewness, R =1e3)
#    ci <- boot.ci(de,type=c("norm","basic","perc"))
#    ci.norm[i,]<-ci$norm[2:3];ci.basic[i,]<-ci$basic[4:5]
#    ci.perc[i,]<-ci$percent[4:5]
#  }
#  cat('norm =',mean(ci.norm[,1]<=0 & ci.norm[,2]>=0),
#      'basic =',mean(ci.basic[,1]<=0 & ci.basic[,2]>=0),
#      'perc =',mean(ci.perc[,1]<=0 & ci.perc[,2]>=0))

## ----eval=FALSE---------------------------------------------------------------
#  mu<-40/10^1.5;n<-1e2;m<-1e3;library(boot);set.seed(5)
#  boot.skewness <- function(x,i) mean(((x[i]-mean(x[i]))/sd(x[i]))^3)
#  ci.norm<-ci.basic<-ci.perc<-matrix(0,m,2)
#  for(i in 1:m){
#    U<-rchisq(n,5)
#    de <- boot(data=U,statistic=boot.skewness, R =1e3)
#    ci <- boot.ci(de,type=c("norm","basic","perc"))
#    ci.norm[i,]<-ci$norm[2:3];ci.basic[i,]<-ci$basic[4:5]
#    ci.perc[i,]<-ci$percent[4:5]
#  }
#  cat('norm =',mean(ci.norm[,1]<=mu & ci.norm[,2]>=mu),
#      'basic =',mean(ci.basic[,1]<=mu & ci.basic[,2]>=mu),
#      'perc =',mean(ci.perc[,1]<=mu & ci.perc[,2]>=mu))

## -----------------------------------------------------------------------------
#data set from chickwts of R-studio
attach(chickwts)
x <- as.vector(weight[feed == "linseed"])
y <- as.vector(weight[feed == "casein"])
detach(chickwts)
z<-c(x,y)

## ----echo=FALSE---------------------------------------------------------------
plot(x,ylab='Weight',ylim=range(z),pch=1)
points(y,col=2,pch=2)
legend(8,410,c('linseed','casein'),col=1:2,pch=1:2)

## -----------------------------------------------------------------------------
R <- 1e3;K <- 1:length(z);n<-length(x);set.seed(1)
reps <- numeric(R);t0 <- cor(x,y,method = 'spearman')
for (i in 1:R) {
  k <- sample(K, size = n, replace = FALSE)
  x1 <- z[k]; y1 <- z[-k] #complement of x1
  reps[i] <- cor(x1,y1,method = 'spearman')
}
p <- mean(abs(c(t0, reps)) >= abs(t0))
round(c(p,cor.test(x,y,method = 'spearman')$p.value),3)

## ----eval=FALSE---------------------------------------------------------------
#  library(RANN);library(boot);library(Ball);library(energy)
#  #Tn function to compute the proportion of the first through Kth NN coincidences
#  Tn <- function(z, ix, sizes,k) {
#    n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
#    if(is.vector(z)) z <- data.frame(z);
#    z <- z[ix, ];
#    NN <- nn2(data=z, k=k+1)
#    block1 <- NN$nn.idx[1:n1,-1]
#    block2 <- NN$nn.idx[(n1+1):n,-1]
#    i1 <- sum(block1 <= n1); i2 <- sum(block2 > n1)
#    (i1 + i2) / (k * n)
#  }
#  #eqdist.nn function to compute the p.value of NN-method
#  eqdist.nn <- function(z,sizes,k){
#    boot.obj <- boot(data=z,statistic=Tn,R=R,
#    sim = "permutation", sizes = sizes,k=k)
#    ts <- c(boot.obj$t0,boot.obj$t)
#    p.value <- mean(ts>=ts[1])
#    list(statistic=ts[1],p.value=p.value)
#  }
#  M <- 1e3; R<-1e3; k<- 2; alpha <- 0.05; set.seed(2)
#  n1 <- n2 <- 20; p<-3;n <- n1+n2; N = c(n1,n2)
#  p.values <- matrix(0,M,3)
#  for (i in 1:4) {
#     for(m in 1:M){
#        #Unequal variances and equal expectations
#        #x obey N(0,1),y obey N(0,1.5)
#        if(i==1) {
#           x<-matrix(rnorm(n1*p),ncol=p)
#           y <-matrix(rnorm(n2*p,0,1.5),ncol=p)
#        }
#        #Unequal variances and unequal expectations
#        #x obey N(1,2),y obey N(2,3)
#        else if(i==2) {
#           x<-matrix(rnorm(n1*p,1,2),ncol=p)
#           y <-matrix(rnorm(n2*p,2,3),ncol=p)
#        }
#        #Non-normal distributions:
#        #x obey t(1),y obey  bimodel distribution (mixture of N(0,1) and N(2,3) with probability 0.3 and 0.7 respectively)
#        else if(i==3) {
#           r<-sample(c(0,1),n2*p,replace = TRUE,prob = c(0.3,0.7))
#           x<-matrix(rt(n1*p,1),ncol=p)
#           y <-matrix(r*rnorm(n2*p,0,1)+(1-r)*rnorm(n2*p,2,3),ncol=p)
#        }
#        #Unbalanced samples:
#        #x,y iid obey N(0,1) where x has 10 sanples and y has 30
#        else {
#           n1 <- 10;n2 <- 30;n <- n1+n2; N = c(n1,n2)
#           x <-matrix(rnorm(n1*p),ncol=p)
#           y <-matrix(rnorm(n2*p),ncol=p)
#        }
#        z <- rbind(x,y)
#        p.values[m,1] <- eqdist.nn(z,N,k)$p.value
#        p.values[m,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
#        p.values[m,3] <- bd.test(x=x,y=y,num.permutations=R,seed=m*2)$p.value
#     }
#     cat('The answer of',i,'is:','\n')
#     cat('nn=',mean(p.values[,1]<alpha),
#         'energy=',mean(p.values[,2]<alpha),
#         'ball-methods=',mean(p.values[,3]<alpha),'\n')
#  }
#  

## -----------------------------------------------------------------------------
set.seed(10)
m <- 1e4;R<-4;sigma<-c(1,3,5,7);k <- numeric(R)
Y <- matrix(0,nrow = R,ncol = m);Y[,1] <- c(5,10,15,20)
#generate MC
for (r in 1:R) {
  u <- runif(m)
  for (i in 2:m) {
  y <- rnorm(1,Y[r,i-1],sigma[r])
  if (u[i] <= dt(y,df=1)/dt(Y[r,i-1],df=1)){
    Y[r,i] <- y   #y is accept
    } 
  else {
    Y[r,i] <- Y[r,i-1]  #y is rejected
    k[r] <- k[r]+1      #compute the number of rejection
    }
  }
}

## ----echo=FALSE---------------------------------------------------------------
#plot Y[8000] to Y[8500]
index <- 8000:8500
y1 <- Y[,index]
for (r in 1:R){
  if(r==1){
    plot(index,y1[r,], type="l",main="",xlab='Index',
         col=1,ylab="",ylim = c(-10,10))
    }else{
      lines(index,y1[r,], col=r)
    }
}
legend('topleft',c('Sigma=1','Sigma=3','Sigma=5','Sigma=7'),col=1:R,lwd=2)

## ----eval=FALSE---------------------------------------------------------------
#  b <- 1000     #discard the first 1000 sample
#  Ycut <- Y[,(b+1):m]
#  a <- c(.05, seq(.1, .9, .1), .95)
#  QSample<-matrix(0,nrow = R,ncol = length(a))
#  QSCD <- qt(a,df=1)  #true quantiles of standard Cauchy distribution
#  for (r in 1:R) {
#    QSample[r,] <- quantile(Ycut[r,], a) #sample quantiles of discard the first 1000 of the chain
#  }
#  #make table of quantiles
#  tableQ <- data.frame(round(cbind(QSCD,QSample[1,],
#                        QSample[2,],QSample[3,],QSample[4,]), 5))
#  names(tableQ) <- c('True','Sigma=1','Sigma=3','Sigma=5','Sigma=7')
#  knitr::kable(tableQ,'pipe')

## -----------------------------------------------------------------------------
set.seed(2);N <- 1e4;burn <- 1e3
Z <- matrix(0, N, 2)
a<-4;b<-4;n<-10
###### generate the chain #####
Z[1, ] <- c(sample(1:n,1), runif(1)) #initialize
for (i in 2:N) {
y <- Z[i-1, 2]
Z[i, 1] <- rbinom(1,n,y)
x <- Z[i, 1]
Z[i, 2] <- rbeta(1,x+a,n-x+b)
}
plot(c((8*burn+1):N),Z[(8*burn + 1):N,1],type='l',
     col=1,lwd=2,xlab='Index',ylab='Random numbers')
lines(c((8*burn+1):N),Z[(8*burn + 1):N,2],col=2,lwd=2)
legend('topright',c(expression(x),expression(y)),col=1:2,lwd=2)

## -----------------------------------------------------------------------------
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)

  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}


## ----echo=FALSE---------------------------------------------------------------
#compute diagnostic statistics
psi <- t(apply(Y, 1, cumsum))
for (i in 1:nrow(psi)){
  psi[i,] <- psi[i,] / (1:ncol(psi))
}

#plot psi for the four chains
for (i in 1:R){
  if(i==1){
    plot((4*burn+1):m,psi[i,(4*burn+1):m],type="l",
            ylim = c(-1,1),xlab='Index', ylab=bquote(phi),col=i)
    }else{
      
      lines((4*burn+1):m,psi[i,(4*burn+1):m], col=i)
    }
}
legend('topleft',c('Sigma=1','Sigma=3','Sigma=5','Sigma=7'),col=1:R,lwd=2)
par(mfrow=c(1,1)) #restore default

## -----------------------------------------------------------------------------
#plot the sequence of R-hat statistics
rhat <- rep(0, m)
for (j in (burn+1):m)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(burn+1):m], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)

## -----------------------------------------------------------------------------
psi <- apply(Z, 1, cumsum)
for (i in 1:nrow(psi)){
  psi[i,] <- psi[i,] / (1:ncol(psi))
}
rhat <- rep(0, m)
for (j in (burn+1):m)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(burn+1):m], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)

## -----------------------------------------------------------------------------
sumf <- function(k,d,a) {
  f<-numeric(k+1)
  for (i in 0:k) {
    x1<-(-1)^i/(factorial(i)*2^i)
    x2<-(sum(a^(2*i+2)))^(1/(2*i+2))/(2*i+1)/(2*i+2)
    x3<-gamma((d+1)/2)*gamma(i+3/2)/gamma(i+d/2+1)
    f[i+1]<-x1*x2*x3
    if(f[i+1]<=1e-40) break
  }
  return(sum(f))
}

## -----------------------------------------------------------------------------
k<-1e4;d<-4;a<-c(1,2)
sumf(k,d,a)

## -----------------------------------------------------------------------------
k<-4
cof<-function(k) {2*exp(lgamma(k/2)-lgamma((k-1)/2))/sqrt(pi*(k-1))}
der<-function(alpha,k) {sqrt((k-1)/(k-alpha^2))*(1-alpha^2/k)^(k/2-1)}
eq<-function(alpha){
  eqleft<-cof(k)*der(alpha = alpha,k=k)
  eqright<-cof(k+1)*der(alpha = alpha,k=k+1)
  eqleft-eqright
}
solution <- uniroot(eq,c(0.2,1.3))
round(unlist(solution),5)[1:3]

## -----------------------------------------------------------------------------
a<-seq(-2,2,0.2)
for (i in 1:length(a)) {
  print(eq(a[i]))
}

## -----------------------------------------------------------------------------
N<-1e3;tol <- .Machine$double.eps^0.5
y<-c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)
yhat<-c(0.54, 0.48, 0.33, 0.43, 0.91, 0.21, 0.85)
lambda<-7/sum(y);lambdahat<-1.5;lambdahat.old<-lambdahat+1
for (j in 1:N) {
  lambdahat<-10/(sum(yhat)+3*(lambdahat+1)/lambdahat)
  if (sum(abs(lambdahat - lambdahat.old)/lambdahat.old) < tol) break
  lambdahat.old <- lambdahat
}
print(list(lambda=lambda, lambdahat = lambdahat, iter = j, bias = tol))

## -----------------------------------------------------------------------------
set.seed(1)
trims <- c(0, 0.1, 0.2, 0.5)
x <- rcauchy(100)
cbind(lapply(trims, function(trim) mean(x, trim = trim)),  #code 1
lapply(trims, mean, x = x))  #code 2

## -----------------------------------------------------------------------------
formulas <- list(mpg ~ disp,mpg ~ I(1 / disp),
                 mpg ~ disp + wt,mpg ~ I(1 / disp) + wt) #Four fitting formulas
lms<-lapply(formulas,lm,data=mtcars) #Four parameter results of fitting
rsq <- function(mod) summary(mod)$r.squared #Four r.squared
rsqs<-rbind(lapply(lms,rsq))
rsqs

## -----------------------------------------------------------------------------
set.seed(2)
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})
lms.boot<-lapply(bootstraps,function(data) lm(mpg ~ disp,data = data))
rsqs<-rbind(lapply(lms.boot,rsq))
rsqs

## -----------------------------------------------------------------------------
set.seed(3)
X<-data.frame(replicate(5,rnorm(5e2)))
round(vapply(X,sd,
          FUN.VALUE=c(sd=0)),5)

## -----------------------------------------------------------------------------
set.seed(4)
summary <- function(x) {
  funs <- c(sd)
  sapply(funs, function(f) f(x, na.rm = TRUE))
}
Y<-data.frame(replicate(5,sample(c(1:20,NA),5e2,rep=T)))
round(vapply(Y,summary,
          FUN.VALUE=c(sd=0)),3)

## -----------------------------------------------------------------------------
library(parallel)
cl <- makeCluster(getOption("cl.cores", 2))
xx <- 1
clusterExport(cl, "xx")
parSapply(cl, 1:20, get("+"), 3)

## ----eval=FALSE---------------------------------------------------------------
#  set.seed(1);library(Rcpp)
#  N <- 1e4;a<-4;b<-4;n<-10 #Initial value
#  source('chainR.R')
#  sourceCpp('chainC.cpp')
#  chainsC<-chainC(N,a,b,n);chainsR<-chainR(N,a,b,n)
#  #plot part of image
#  burn<-N/10
#  plot(c((8*burn+1):N),chainsC[(8*burn + 1):N,1],type='l',
#       col=1,lwd=2,xlab='Index',ylab='Random numbers')
#  lines(c((8*burn+1):N),chainsC[(8*burn + 1):N,2],col=2,lwd=2)
#  legend('topright',c(expression(x),expression(y)),col=1:2,lwd=2)

## ----eval=FALSE---------------------------------------------------------------
#  par(mfrow=c(1,2))
#  #different x generated by C and R respectively
#  qqplot(chainsC[,1],chainsR[,1],ylab = 'R_x',xlab = 'C_x',main='(a)')
#  #different y generated by C and R respectively
#  qqplot(chainsC[,2],chainsR[,2],ylab = 'R_y',xlab = 'C_y',main='(b)')

## ----eval=FALSE---------------------------------------------------------------
#  library(microbenchmark) #My R version is 4.1.3
#  ts <- microbenchmark(chainR=chainR(N,a,b,n), chainC=chainC(N,a,b,n))
#  summary(ts)[,c(1,3,5,6)]

